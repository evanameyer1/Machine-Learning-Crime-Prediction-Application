{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in datasets\n",
    "clean_crime = pd.read_csv('../clean_datasets/clean_crime.csv')\n",
    "area_reference = pd.read_csv('../scoring_datasets/area_reference.csv')\n",
    "clean_ridership = pd.read_csv('../clean_datasets/clean_ridership.csv')\n",
    "clean_lighting = pd.read_csv('../clean_datasets/clean_lighting.csv')\n",
    "clean_divvy_trips = pd.read_csv('../clean_datasets/clean_divvy_trips.csv')\n",
    "clean_vacant_buildings = pd.read_csv('../clean_datasets/clean_vacant_buildings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df):\n",
    "    all_dates = pd.date_range(start='2001-01-01', end='12/31/2022', freq='D')\n",
    "    all_hours = range(1, 24)\n",
    "    all_areas = range(1, 78)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    date_hour_combinations = pd.DataFrame(([datetime.datetime.strptime(str(date)[0:10], \"%Y-%m-%d\"), hour, area] for date in all_dates for hour in all_hours for area in all_areas), columns=['date', 'hour', 'area'])\n",
    "    merged_df = date_hour_combinations.merge(df, on=['date', 'hour', 'area'], how='outer').fillna(0)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data():\n",
    "\n",
    "    #clean the crime data so the aggregation process is smoother\n",
    "    clean_crime['date'] = [datetime.datetime.strptime(clean_crime.date.iloc[i], '%m/%d/%Y %I:%M:%S %p') for i in range(len(clean_crime))]\n",
    "    clean_crime['hour'] = clean_crime.date.apply(lambda x : (x.hour + 1))\n",
    "    clean_crime['date'] = clean_crime.date.apply(lambda x : x.date())\n",
    "\n",
    "    violent_crimes = [ 'BATTERY', 'ASSAULT', 'CRIM SEXUAL ASSAULT', 'SEX OFFENSE', 'WEAPONS VIOLATION',\n",
    "    'HOMICIDE', 'KIDNAPPING', 'ROBBERY', 'INTIMIDATION', 'ARSON', 'CRIMINAL SEXUAL ASSAULT', 'HUMAN TRAFFICKING']\n",
    "\n",
    "    clean_crime['type'] = clean_crime['type'].apply(lambda x : 2 if x in violent_crimes else 1)\n",
    "\n",
    "    print('clean_crime successfully read in')\n",
    "\n",
    "    #group the crime dataset utilizing a count aggregation\n",
    "    grouped_crime = clean_crime.groupby(['date', 'hour', 'type', 'area']).size().reset_index(name='count')\n",
    "\n",
    "    #pivot the violent vs non-violent counts to display across our grouped columns\n",
    "    grouped_crime = grouped_crime.pivot_table(index=['date', 'hour', 'area'], columns='type', values='count', fill_value=0).reset_index()\n",
    "    grouped_crime.rename(columns={1 : \"non-violent\", 2 : 'violent'}, inplace=True)\n",
    "\n",
    "    print('crime dataset successfully grouped')\n",
    "\n",
    "    #call fill_gaps function\n",
    "    grouped_crime = fill_gaps(grouped_crime)\n",
    "\n",
    "    print('fill gaps function successfully ran in')\n",
    "    \n",
    "    #merge the area_reference dataset on top of the grouped_crime data\n",
    "    grouped_crime = grouped_crime.merge(area_reference, left_on='area', right_on='id', how='left')\n",
    "    grouped_crime.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    print('area reference data successfully merged')\n",
    "    \n",
    "    #match datatypes of columns and then merge the two datasets together\n",
    "    grouped_crime['date'] = pd.to_datetime(grouped_crime['date'])\n",
    "    clean_ridership['date'] = pd.to_datetime(clean_ridership['date'])\n",
    "    grouped_crime = grouped_crime.merge(clean_ridership, on=['date', 'area'], how='left')\n",
    "    \n",
    "    print('clean ridership data successfully merged')\n",
    "\n",
    "    #match datatypes of columns and then group the dataset to make merging together easier\n",
    "    clean_divvy_trips['hour'] = pd.to_datetime(clean_divvy_trips['date']).dt.hour\n",
    "    clean_divvy_trips['date'] = pd.to_datetime(pd.to_datetime(clean_divvy_trips['date']).dt.date)\n",
    "    grouped_divvy = clean_divvy_trips.groupby(['date', 'hour', 'area'])['station_id'].agg('count').reset_index()  \n",
    "\n",
    "    #merge the grouped divvy data onto the base crime data  \n",
    "    grouped_crime = grouped_crime.merge(grouped_divvy, on=['date', 'hour', 'area'], how='left')\n",
    "    grouped_crime.rename(columns={'station_id' : 'bike_rides', 'rides' : 'train_rides'}, inplace=True)\n",
    "\n",
    "    print('clean divvy trips data successfully merged')\n",
    "\n",
    "    grouped_lighting = clean_lighting.groupby(['date', 'area'])['lat'].agg('count').reset_index()\n",
    "    grouped_lighting.date = pd.to_datetime(grouped_lighting.date)\n",
    "    grouped_crime = grouped_crime.merge(grouped_lighting, on=['date', 'area'], how='left')\n",
    "    grouped_crime.rename(columns={'lat' : 'lighting'}, inplace=True)\n",
    "\n",
    "    print('clean lighting data successfully merged')\n",
    "\n",
    "    clean_vacant_buildings.date = pd.to_datetime(clean_vacant_buildings.date)\n",
    "    grouped_vacancies = clean_vacant_buildings[clean_vacant_buildings.date <= pd.to_datetime('2022-12-31')].groupby(['date', 'area'])['long'].agg('count').reset_index()\n",
    "    grouped_crime = grouped_crime.merge(grouped_vacancies, on=['date', 'area'], how='left')\n",
    "    grouped_crime.rename(columns={'long' : 'vacant_buildings'}, inplace=True)\n",
    "\n",
    "    print('clean vacancies data successfully merged')\n",
    "\n",
    "    return grouped_crime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_crime successfully read in\n",
      "crime dataset successfully grouped\n",
      "fill gaps function successfully ran in\n",
      "area reference data successfully merged\n",
      "clean ridership data successfully merged\n",
      "clean divvy trips data successfully merged\n",
      "clean lighting data successfully merged\n",
      "clean vacancies data successfully merged\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>area</th>\n",
       "      <th>non-violent</th>\n",
       "      <th>violent</th>\n",
       "      <th>cta_stations</th>\n",
       "      <th>police_stations</th>\n",
       "      <th>bus_stations</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>per_capita_income</th>\n",
       "      <th>no_hs_dip</th>\n",
       "      <th>gov_depend</th>\n",
       "      <th>crowded_housing</th>\n",
       "      <th>below_pov</th>\n",
       "      <th>bike_stations</th>\n",
       "      <th>train_rides</th>\n",
       "      <th>bike_rides</th>\n",
       "      <th>lighting</th>\n",
       "      <th>vacant_buildings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>23714.0</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.227</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>21375.0</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.151</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.077</td>\n",
       "      <td>32355.0</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.227</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.068</td>\n",
       "      <td>35503.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.095</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>51615.0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.071</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611310</th>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.057</td>\n",
       "      <td>60593.0</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.111</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611311</th>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.173</td>\n",
       "      <td>18928.0</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.283</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611312</th>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.177</td>\n",
       "      <td>18366.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611313</th>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>20320.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.253</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611314</th>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>12524.0</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.306</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14611315 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  hour  area  non-violent  violent  cta_stations  \\\n",
       "0        2001-01-01     1     1          4.0      0.0           4.0   \n",
       "1        2001-01-01     1     2          3.0      0.0           0.0   \n",
       "2        2001-01-01     1     3          5.0      2.0           3.0   \n",
       "3        2001-01-01     1     4          2.0      0.0           4.0   \n",
       "4        2001-01-01     1     5          1.0      0.0           2.0   \n",
       "...             ...   ...   ...          ...      ...           ...   \n",
       "14611310 2023-10-10     1    33          1.0      0.0           2.0   \n",
       "14611311 2023-10-10     1    42          1.0      0.0           2.0   \n",
       "14611312 2023-10-10     1    43          1.0      0.0           0.0   \n",
       "14611313 2023-10-10     1    44          1.0      0.0           2.0   \n",
       "14611314 2023-10-10     1    61          0.0      1.0           0.0   \n",
       "\n",
       "          police_stations  bus_stations  unemployment  per_capita_income  \\\n",
       "0                     1.0           7.0         0.075            23714.0   \n",
       "1                     0.0           2.0         0.079            21375.0   \n",
       "2                     0.0           3.0         0.077            32355.0   \n",
       "3                     1.0           5.0         0.068            35503.0   \n",
       "4                     0.0           5.0         0.045            51615.0   \n",
       "...                   ...           ...           ...                ...   \n",
       "14611310              1.0           7.0         0.057            60593.0   \n",
       "14611311              1.0           9.0         0.173            18928.0   \n",
       "14611312              0.0          10.0         0.177            18366.0   \n",
       "14611313              0.0          10.0         0.190            20320.0   \n",
       "14611314              0.0           2.0         0.174            12524.0   \n",
       "\n",
       "          no_hs_dip  gov_depend  crowded_housing  below_pov  bike_stations  \\\n",
       "0             0.181       0.288            0.079      0.227           17.0   \n",
       "1             0.196       0.383            0.070      0.151           30.0   \n",
       "2             0.136       0.222            0.046      0.227           15.0   \n",
       "3             0.125       0.256            0.031      0.095           19.0   \n",
       "4             0.054       0.255            0.002      0.071           15.0   \n",
       "...             ...         ...              ...        ...            ...   \n",
       "14611310      0.071       0.210            0.014      0.111           15.0   \n",
       "14611311      0.179       0.376            0.018      0.283           22.0   \n",
       "14611312      0.149       0.376            0.029      0.315           23.0   \n",
       "14611313      0.137       0.400            0.022      0.253           33.0   \n",
       "14611314      0.424       0.420            0.122      0.306           31.0   \n",
       "\n",
       "          train_rides  bike_rides  lighting  vacant_buildings  \n",
       "0                 NaN         NaN       NaN               NaN  \n",
       "1                 NaN         NaN       NaN               NaN  \n",
       "2                 NaN         NaN       NaN               NaN  \n",
       "3                 NaN         NaN       NaN               NaN  \n",
       "4                 NaN         NaN       NaN               NaN  \n",
       "...               ...         ...       ...               ...  \n",
       "14611310          NaN         NaN       NaN               NaN  \n",
       "14611311          NaN         NaN       NaN               NaN  \n",
       "14611312          NaN         NaN       NaN               NaN  \n",
       "14611313          NaN         NaN       NaN               NaN  \n",
       "14611314          NaN         NaN       NaN               NaN  \n",
       "\n",
       "[14611315 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_aggregation = aggregate_data()\n",
    "final_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_aggregation.to_csv('../scoring_datasets/final_aggregation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
