{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_tables(n=78):\n",
    "    df_list = {}\n",
    "    for i in range(1, n):\n",
    "\n",
    "        table1_name = 'test_df' + str(i)\n",
    "        table2_name = 'train_df' + str(i)\n",
    "\n",
    "        df = pd.read_csv(f'../datasets/training/SARIMA/by_area/ml_{i}.csv', parse_dates=['date'])\n",
    "\n",
    "        globals()[table1_name] = df[(df.date <= pd.to_datetime('2019-12-31')) & (df.date >= pd.to_datetime('2019-01-01'))]\n",
    "        globals()[table2_name] = df[(df.date < pd.to_datetime('2019-01-01')) & (df.date > pd.to_datetime('2016-01-01'))]\n",
    "\n",
    "        globals()[table1_name].fillna(0, inplace=True)\n",
    "        globals()[table2_name].fillna(0, inplace=True)\n",
    "\n",
    "        globals()[table1_name].set_index('date', inplace=True)\n",
    "        globals()[table2_name].set_index('date', inplace=True)\n",
    "\n",
    "        df_list[table1_name] = globals()[table1_name]\n",
    "        df_list[table2_name] = globals()[table2_name]\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = read_in_tables()\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_columns(df, target_cols):\n",
    "    \"\"\"\n",
    "    Create new columns for each numeric column with aggregated values of the previous 6, 12, and 24 hours.\n",
    "    Fill nulls with zeros after the initial nulls when the rolling window size hasn't been met yet.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with datetime index and numeric columns to be aggregated\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with new aggregated columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in target_cols:\n",
    "        # Create new columns with aggregated values\n",
    "        df_copy[f'{column}_6h_agg'] = df_copy[column].rolling(window=6, min_periods=1).sum().fillna(0)\n",
    "        df_copy[f'{column}_12h_agg'] = df_copy[column].rolling(window=12, min_periods=1).sum().fillna(0)\n",
    "        df_copy[f'{column}_24h_agg'] = df_copy[column].rolling(window=24, min_periods=1).sum().fillna(0)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_df = create_aggregated_columns(train_df1, ['non-violent', 'violent', 'bike_rides'])\n",
    "temp_test_df = create_aggregated_columns(test_df1, ['non-violent', 'violent', 'bike_rides'])\n",
    "temp_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    # Class-level attributes for model coefficients and parameters\n",
    "    m = []  # Model coefficients for features\n",
    "    b = 0   # Model intercept\n",
    "    initial_learning_rate = 0.01  # Initial learning rate (how fast the model moves the coefficients)\n",
    "    learning_rate = initial_learning_rate\n",
    "    learning_rate_decay = 0.99    # Learning rate decay factor (over time the learning rate will shrink at this rate)\n",
    "    epochs = 100  # Number of training epochs (maximum number of iterations to run the model through before finding the values corresponding with the minimum error)\n",
    "    data = None \n",
    "    test_dataset = None\n",
    "    train_dataset = None\n",
    "    length = 0\n",
    "    feature_columns = [] # Input columns\n",
    "    target_column = \"\" # Output column\n",
    "    losses = {}  # Store loss values during training\n",
    "\n",
    "    def __init__(self, dataset, feature_columns, target_column, initial_learning_rate=0.01, epochs=100, learning_rate_decay=0.99):\n",
    "        \"\"\"\n",
    "        Initialize the LinearRegression model.\n",
    "\n",
    "        Parameters:\n",
    "        - dataset: pandas DataFrame, the dataset to train and test the model.\n",
    "        - feature_columns: list of str, column names representing features.\n",
    "        - target_column: str, the column name representing the target variable.\n",
    "        - initial_learning_rate: float, the initial learning rate for gradient descent.\n",
    "        - epochs: int, the number of training epochs.\n",
    "        - learning_rate_decay: float, the factor by which learning rate decays at each epoch.\n",
    "        \"\"\"\n",
    "        # Define class properties\n",
    "        self.data = dataset\n",
    "        self.feature_columns = feature_columns\n",
    "        self.target_column = target_column\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.epochs = epochs\n",
    "        self.length = len(dataset)\n",
    "\n",
    "        # Make a unique coefficient value for each feature column\n",
    "        for feature in feature_columns:\n",
    "            self.m.append(0)\n",
    "\n",
    "    def loss(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error loss of the model on a dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pandas DataFrame, the dataset on which to calculate the loss.\n",
    "\n",
    "        Returns:\n",
    "        - str: A string representing the total error normalized by the mean target value.\n",
    "        \"\"\"\n",
    "        # Initialize our variables\n",
    "        total_error = 0\n",
    "        total_y = 0\n",
    "\n",
    "        # Iterating through every row of the passed dataframe\n",
    "        for i in range(len(df)):\n",
    "            \n",
    "            # Define our x values as a vector of each feature column at the current row\n",
    "            x = [df.iloc[i][col] for col in self.feature_columns]\n",
    "\n",
    "            # Grab our expected y value to compare our predicted y value against\n",
    "            y = df.iloc[i][self.target_column]\n",
    "\n",
    "            # Sum the products of each coefficient and corresponding x value \n",
    "            total_m = sum(self.m[j] * x[j] for j in range(len(self.feature_columns)))\n",
    "            total_y += (total_m + self.b)\n",
    "\n",
    "            # Calculate squared difference\n",
    "            total_error += (y - (total_m + self.b)) ** 2\n",
    "\n",
    "        # Average values over each row\n",
    "        total_error /= float(len(df))\n",
    "        total_y /= float(len(df))\n",
    "\n",
    "        return total_error, f'Total Error for the Model: {total_error} | Relative Error for the Model: {total_error / abs(total_y)}'\n",
    "\n",
    "    def gradient_descent(self, m_now, b_now):\n",
    "        \"\"\"\n",
    "        Perform one step of gradient descent to update model coefficients.\n",
    "\n",
    "        Parameters:\n",
    "        - m_now: list of float, current model coefficients for features.\n",
    "        - b_now: float, current model intercept.\n",
    "\n",
    "        Returns:\n",
    "        - list of float: Updated model coefficients for features.\n",
    "        - float: Updated model intercept.\n",
    "        \"\"\"\n",
    "        # Initalize a gradient variable for each feature column\n",
    "        m_gradients = [0] * (len(self.feature_columns))\n",
    "        b_gradient = 0\n",
    "\n",
    "        # Iterate through every row\n",
    "        for i in range(self.length):\n",
    "\n",
    "            # Define our x values as a vector of each feature column at the current row\n",
    "            x = [self.train_dataset.iloc[i][col] for col in self.feature_columns]\n",
    "\n",
    "            # Grab our expected y value to compare our predicted y value against\n",
    "            y = self.train_dataset.iloc[i][self.target_column]\n",
    "\n",
    "            # Sum the products of each coefficient and corresponding x value \n",
    "            total_m = sum(m_now[j] * x[j] for j in range(len(self.feature_columns)))\n",
    "\n",
    "            # Take the derivative with respect to b\n",
    "            b_gradient += -(2 / self.length) * (y - (total_m + b_now))\n",
    "\n",
    "            # Take the derivative with respect to m\n",
    "            for j in range(len(self.feature_columns)):\n",
    "                m_gradients[j] += -(2 / self.length) * x[j] * (y - (total_m + b_now))\n",
    "\n",
    "        # Updated the model's coefficients based on the gradient values\n",
    "        updated_m = [m_now[j] - m_gradients[j] * self.learning_rate for j in range(len(self.feature_columns))]\n",
    "        updated_b = b_now - b_gradient * self.learning_rate\n",
    "\n",
    "        return updated_m, updated_b\n",
    "\n",
    "    def train(self, ratio=0.1, max_epochs=None, min_loss_delta=0.01, patience=10):\n",
    "        \"\"\"\n",
    "            Train the Linear Regression model using gradient descent.\n",
    "\n",
    "            This method iteratively updates the model's coefficients and intercept using gradient descent. It monitors the training process for convergence and stops training based on specified conditions, such as the maximum number of epochs, loss improvement, or patience.\n",
    "\n",
    "            Parameters:\n",
    "            - ratio (float): The fraction of the maximum epochs used for checkpointing.\n",
    "            - max_epochs (int): The maximum number of training epochs.\n",
    "            - min_loss_delta (float): The minimum change in loss to continue training.\n",
    "            - patience (int): The number of epochs to tolerate a lack of improvement.\n",
    "\n",
    "            Returns:\n",
    "            - None\n",
    "\n",
    "            Raises:\n",
    "            - ValueError: If max_epochs is not provided or is less than or equal to 0.\n",
    "\n",
    "            This method also sets the model's parameters to the values associated with the minimum stored loss during training.\n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "        if max_epochs is None:\n",
    "            raise ValueError(\"You must provide a value for max_epochs.\")\n",
    "        if max_epochs <= 0:\n",
    "            raise ValueError(\"max_epochs must be greater than 0.\")\n",
    "        \n",
    "        # Initialize variables\n",
    "        checkpoint = round(max_epochs * ratio)\n",
    "        learning_rate = self.initial_learning_rate\n",
    "        prev_loss = float('inf')\n",
    "        min_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(max_epochs):\n",
    "            # Save coefficients in case we want to revert back after calculating error\n",
    "            temp_m = self.m\n",
    "            temp_b = self.b\n",
    "\n",
    "            # Determine gradient descent\n",
    "            self.m, self.b = self.gradient_descent(self.m, self.b)\n",
    "\n",
    "            # Determine the loss of the updated model \n",
    "            loss, string = self.loss(self.train_dataset[:i + 1])\n",
    "            self.losses[loss] = self.m, self.b\n",
    "\n",
    "            # If the error loss is increasing by a statistically determined amount add one to the patience counter, which will break the loop when it consecutively reaches the threshold\n",
    "            if (prev_loss != float('inf')) and ((loss - prev_loss >= prev_loss * 0.5) or (loss >= min_loss)) and (loss < statistics.stdev(self.train_dataset[self.target_column])):\n",
    "                patience_counter += 1\n",
    "\n",
    "            # If the error is moving down, reset the patience counter and update the minimum loss variable\n",
    "            elif loss < min_loss:\n",
    "                min_loss = loss\n",
    "                patience_counter = 0\n",
    "\n",
    "            # Otherwise the error loss is increasing slightly\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "\n",
    "            # Add the loss to our lost history storage\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Every predetermined checkpoint we print updates\n",
    "            if (i % checkpoint == 0) and (i != 0):\n",
    "                print(f'{time.strftime(\"%Y-%m-%d %H:%M:%S\")} - Epoch {i} - {string}')\n",
    "                print(self.m, self.b)\n",
    "\n",
    "            # If the patience counter has surpassed the patience threshold, exit the loop\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Training stopped due to lack of improvement for {patience} epochs. Current Error - {loss}\")\n",
    "                break\n",
    "\n",
    "            # Updated loss and learning rate variables\n",
    "            prev_loss = loss\n",
    "            learning_rate *= self.learning_rate_decay\n",
    "            self.learning_rate = learning_rate\n",
    "\n",
    "            # Calculate moving average of the loss history\n",
    "            if i >= patience:\n",
    "                recent_losses = loss_history[-patience:]\n",
    "                moving_average = np.mean(recent_losses)\n",
    "                if moving_average < min_loss_delta:\n",
    "                    print(f\"Training stopped due to a small moving average loss change.\")\n",
    "                    break\n",
    "\n",
    "        # Set the parameters to the ones with the minimum stored loss\n",
    "        min_loss = min(self.losses.keys())\n",
    "        self.m, self.b = self.losses[min_loss]\n",
    "        print(f'Training Complete. Final Error: {min_loss}')\n",
    "\n",
    "    def split_datasets(self, frac=0.2):\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing subsets.\n",
    "\n",
    "        Parameters:\n",
    "        - frac: float, the fraction of data to be used for testing.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # Split the primary dataset into a test and training dataset based on the passed fraction\n",
    "        test = self.data.sample(frac=frac)\n",
    "        train = self.data.sample(frac=1-frac)\n",
    "        self.train_dataset = train\n",
    "        self.test_dataset = test\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        - data: pandas DataFrame, the dataset for which to make predictions.\n",
    "\n",
    "        Returns:\n",
    "        - list of float: Predicted values based on the model.\n",
    "        \"\"\"\n",
    "        prediction = []\n",
    "\n",
    "        # Iterate through every row\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            # Create a vector of all x values at the current column\n",
    "            x = [data.iloc[i][col] for col in data.columns]\n",
    "\n",
    "            # Sum the product of all x values and their respective coefficients\n",
    "            total_m = sum(self.m[j] * x[j] for j in range(len(data.columns)))\n",
    "\n",
    "            # Calculate the predicted y-value and add it to the output list\n",
    "            y = total_m + self.b\n",
    "            prediction.append(y)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def test_results(self):\n",
    "        \"\"\"\n",
    "        Calculate and return the loss on the test dataset.\n",
    "\n",
    "        Returns:\n",
    "        - str: A string representing the total error normalized by the mean target value on the test dataset.\n",
    "        \"\"\"\n",
    "        # Return the calculated error loss for the saved test-dataset\n",
    "        return self.loss(self.test_dataset)\n",
    "\n",
    "    def set_train_dataset(self, dataset):\n",
    "        \"\"\"\n",
    "        Set the training dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - dataset: pandas DataFrame, the training dataset.\n",
    "        \"\"\"\n",
    "        self.train_dataset = dataset\n",
    "\n",
    "    def set_test_dataset(self, dataset):\n",
    "        \"\"\"\n",
    "        Set the testing dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - dataset: pandas DataFrame, the testing dataset.\n",
    "        \"\"\"\n",
    "        self.test_dataset = dataset\n",
    "\n",
    "    def reset_losses(self):\n",
    "        \"\"\"\n",
    "        Reset the list of loss values.\n",
    "        \"\"\"\n",
    "        self.losses = {}\n",
    "\n",
    "    def list_to_models(self, data, output=None, initial_learning_rate=0.0001, learning_rate_decay=0.999, epochs=500, max_epochs=None, patience=10):\n",
    "        \"\"\"\n",
    "        Generate a dictionary of models with varying feature columns and training each model.\n",
    "\n",
    "        This method trains multiple Linear Regression models with different sets of feature columns. \n",
    "        It iterates through the provided list of feature columns and for each set of features, \n",
    "        it trains a Linear Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        - data: pandas DataFrame, the dataset to use for training all models.\n",
    "        - output (dict): A dictionary to store the results of each model. If not provided, \n",
    "        a new dictionary will be created.\n",
    "        - initial_learning_rate (float): The initial learning rate for gradient descent.\n",
    "        - learning_rate_decay (float): The factor by which learning rate decays at each epoch.\n",
    "        - epochs (int): The number of training epochs for each model.\n",
    "        - max_epochs (int): Maximum number of epochs to run for each model.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the results and models for each trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        if output is None:\n",
    "            output = {}\n",
    "\n",
    "        # Iterate through the list of feature columns, running a model with an additional feature column each time\n",
    "        for i in range(1, len(self.feature_columns) + 1):\n",
    "            temp_results = {}\n",
    "            model_name = 'temp_model_' + str(i)\n",
    "\n",
    "            # Create a model instance with a subset of feature columns\n",
    "            model_instance = LinearRegression(data, self.feature_columns[:i], 'non-violent', initial_learning_rate, \n",
    "            epochs=(epochs + math.floor(i / 2) * 2 * epochs), learning_rate_decay=learning_rate_decay)\n",
    "\n",
    "            # Set training data as the model's training dataset\n",
    "            model_instance.set_train_dataset(data)\n",
    "\n",
    "            # Train the model with a maximum number of epochs and patience\n",
    "            model_instance.train(max_epochs=epochs, patience=patience)\n",
    "\n",
    "            # Store the results in a dictionary\n",
    "            temp_results['results'] = model_instance.model()\n",
    "            temp_results['model'] = model_instance\n",
    "            output[model_name] = temp_results\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Trained {model_name} with {i} feature columns.\")\n",
    "            print(f\"Model: {model_instance.model()}\")\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def lasso_regularization(self, alpha):\n",
    "        \"\"\"\n",
    "        Apply L1 (Lasso) regularization to the model.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: float, regularization strength.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.m)):\n",
    "            if self.m[i] > 0:\n",
    "                self.m[i] -= alpha\n",
    "            elif self.m[i] < 0:\n",
    "                self.m[i] += alpha\n",
    "        print(\"L1 (Lasso) regularization applied.\")\n",
    "\n",
    "    def ridge_regularization(self, alpha):\n",
    "        \"\"\"\n",
    "        Apply L2 (Ridge) regularization to the model.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: float, regularization strength.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.m)):\n",
    "            self.m[i] -= 2 * alpha * self.m[i]\n",
    "        print(\"L2 (Ridge) regularization applied.\")\n",
    "\n",
    "    def cross_validation(self, k=5):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation to assess model performance.\n",
    "\n",
    "        Parameters:\n",
    "        - k: int, number of folds for cross-validation.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary containing evaluation metrics for each fold.\n",
    "        \"\"\"\n",
    "        # Split data into k folds\n",
    "        fold_size = len(self.data) // k\n",
    "        metrics = {'MAE': [], 'RMSE': [], 'R-squared': []}\n",
    "\n",
    "        for i in range(k):\n",
    "            # Create train and test datasets for the current fold\n",
    "            start = i * fold_size\n",
    "            end = start + fold_size\n",
    "            test_fold = self.data[start:end]\n",
    "            train_fold = pd.concat([self.data[:start], self.data[end:]])\n",
    "\n",
    "            # Train the model on the training fold\n",
    "            self.set_train_dataset(train_fold)\n",
    "            self.train()\n",
    "\n",
    "            # Make predictions on the test fold\n",
    "            predictions = self.predict(test_fold)\n",
    "\n",
    "            # Calculate evaluation metrics for this fold\n",
    "            mae = self.mean_absolute_error(predictions)\n",
    "            rmse = self.root_mean_squared_error(predictions)\n",
    "            r_squared = self.r_squared(predictions)\n",
    "\n",
    "            metrics['MAE'].append(mae)\n",
    "            metrics['RMSE'].append(rmse)\n",
    "            metrics['R-squared'].append(r_squared)\n",
    "            print(f\"Cross-validation fold {i+1} completed.\")\n",
    "        \n",
    "        print(\"Cross-validation completed.\")\n",
    "        return metrics\n",
    "\n",
    "    def standardize_features(self):\n",
    "        \"\"\"\n",
    "        Standardize feature columns using StandardScaler.\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        self.data[self.feature_columns] = scaler.fit_transform(self.data[self.feature_columns])\n",
    "        print(\"Features standardized using StandardScaler.\")\n",
    "\n",
    "    def min_max_scaling(self):\n",
    "        \"\"\"\n",
    "        Apply Min-Max scaling to feature columns.\n",
    "        \"\"\"\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        self.data[self.feature_columns] = min_max_scaler.fit_transform(self.data[self.feature_columns])\n",
    "        print(\"Min-Max scaling applied to feature columns.\")\n",
    "\n",
    "    def mean_absolute_error(self, predictions):\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Error (MAE) for model predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: list of float, predicted values.\n",
    "\n",
    "        Returns:\n",
    "        - float: Mean Absolute Error.\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs(predictions - self.test_dataset[self.target_column]))\n",
    "\n",
    "    def root_mean_squared_error(self, predictions):\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Squared Error (RMSE) for model predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: list of float, predicted values.\n",
    "\n",
    "        Returns:\n",
    "        - float: Root Mean Squared Error.\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.mean((predictions - self.test_dataset[self.target_column]) ** 2))\n",
    "\n",
    "    def r_squared(self, predictions):\n",
    "        \"\"\"\n",
    "        Calculate R-squared (coefficient of determination) for model predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: list of float, predicted values.\n",
    "\n",
    "        Returns:\n",
    "        - float: R-squared.\n",
    "        \"\"\"\n",
    "        ssr = np.sum((predictions - self.test_dataset[self.target_column]) ** 2)\n",
    "        sst = np.sum((self.test_dataset[self.target_column] - np.mean(self.test_dataset[self.target_column])) ** 2)\n",
    "        return 1 - (ssr / sst)\n",
    "\n",
    "    def hyperparameter_tuning(self, parameter_grid, scoring='neg_mean_squared_error', cv=5):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using grid search or random search.\n",
    "\n",
    "        Parameters:\n",
    "        - parameter_grid: dict, a dictionary of hyperparameter values to search.\n",
    "        - scoring: str, the scoring metric to optimize.\n",
    "        - cv: int, the number of cross-validation folds.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary containing the best hyperparameters and their corresponding performance.\n",
    "        \"\"\"\n",
    "        # Create a model instance\n",
    "        model = LinearRegression(self.data, self.feature_columns, self.target_column)\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        grid_search = GridSearchCV(model, parameter_grid, scoring=scoring, cv=cv)\n",
    "        grid_search.fit(self.data[self.feature_columns], self.data[self.target_column])\n",
    "\n",
    "        # Get the best hyperparameters and their corresponding performance\n",
    "        best_hyperparameters = grid_search.best_params_\n",
    "        best_performance = grid_search.best_score_\n",
    "\n",
    "        return {'best_hyperparameters': best_hyperparameters, 'best_performance': best_performance}\n",
    "\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        Generate and return the equation of the trained linear regression model.\n",
    "\n",
    "        Returns:\n",
    "        - str: The equation of the linear regression model.\n",
    "        \"\"\"\n",
    "        model_parts = [f'({self.m[i]} * {col})' for i, col in enumerate(self.feature_columns)]\n",
    "        model_str = ' + '.join(model_parts)\n",
    "\n",
    "        if self.b >= 0:\n",
    "            model = f'y = {model_str} + {self.b}'\n",
    "        else:\n",
    "            temp_b = abs(self.b)\n",
    "            model = f'y = {model_str} - {temp_b}'\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalize all columns in a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A new DataFrame with normalized values.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    means = df.mean()\n",
    "    stds = df.std()\n",
    "    \n",
    "    # Normalize each column\n",
    "    normalized_df = (df - means) / stds\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_df = normalize_dataframe(temp_test_df)\n",
    "temp_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LinearRegression(temp_train_df, ['non-violent_6h_agg', 'non-violent_12h_agg', 'non-violent_24h_agg', 'violent_6h_agg',\t'violent_12h_agg',\t\n",
    "                                              'violent_24h_agg', 'bike_rides_6h_agg', 'bike_rides_12h_agg', 'bike_rides_24h_agg'], 'non-violent', \n",
    "                                              initial_learning_rate=0.0001, learning_rate_decay=.999, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.set_train_dataset(temp_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['final'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test_model.list_to_models(models)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_losses = models['test_model_1']['model'].losses\n",
    "Q1 = np.percentile(model_losses, 25)\n",
    "Q3 = np.percentile(model_losses, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a lower and upper bound\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers based on the bounds\n",
    "outliers_removed = [x for x in model_losses if lower_bound <= x <= upper_bound]\n",
    "\n",
    "plt.plot(range(len(outliers_removed)), outliers_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(1, 78):\n",
    "        model_name = 'model' + str(i)\n",
    "        nv_model = 'nv_model' + str(i)\n",
    "        v_model = 'v_model' + str(i)\n",
    "        train = 'train_df' + str(i)\n",
    "        test = 'test_df' + str(i)\n",
    "\n",
    "        output = {}\n",
    "        nv_models = {}\n",
    "        v_models = {}\n",
    "\n",
    "        temp_train_df = create_aggregated_columns(tables[train], ['non-violent', 'violent', 'bike_rides'])\n",
    "        temp_test_df = create_aggregated_columns(tables[test], ['non-violent', 'violent', 'bike_rides'])\n",
    "\n",
    "        temp_train_df = normalize_dataframe(temp_train_df)\n",
    "        temp_test_df = normalize_dataframe(temp_test_df)\n",
    "\n",
    "        print(f\"Training non-violent model for {model_name}\")\n",
    "        globals()[nv_model] = LinearRegression(temp_train_df, ['non-violent_6h_agg', 'non-violent_12h_agg', 'non-violent_24h_agg', 'violent_6h_agg', 'violent_12h_agg',\n",
    "                                                    'violent_24h_agg', 'bike_rides_6h_agg', 'bike_rides_12h_agg', 'bike_rides_24h_agg'], 'non-violent', \n",
    "                                                    initial_learning_rate=0.0001, learning_rate_decay=.999, epochs=10000)\n",
    "\n",
    "        globals()[nv_model].set_train_dataset(temp_train_df)\n",
    "        globals()[nv_model].set_test_dataset(temp_test_df)\n",
    "\n",
    "        nv_output = globals()[nv_model].list_to_models(temp_train_df, nv_models, patience=100)\n",
    "        output['nv-model'] = globals()[nv_model]\n",
    "        output['non-violent'] = nv_models\n",
    "\n",
    "        trained_datasets[model_name] = output\n",
    "\n",
    "        print(f\"Training violent model for {model_name}\")\n",
    "        globals()[v_model] = LinearRegression(temp_train_df, ['non-violent_6h_agg', 'non-violent_12h_agg', 'non-violent_24h_agg', 'violent_6h_agg', 'violent_12h_agg',\n",
    "                                                    'violent_24h_agg', 'bike_rides_6h_agg', 'bike_rides_12h_agg', 'bike_rides_24h_agg'], 'violent', \n",
    "                                                    initial_learning_rate=0.0001, learning_rate_decay=.999, epochs=10000)\n",
    "        \n",
    "        globals()[v_model].set_train_dataset(temp_train_df)\n",
    "        globals()[v_model].set_test_dataset(temp_test_df)\n",
    "\n",
    "        v_output = globals()[v_model].list_to_models(temp_train_df, v_models, patience=100)\n",
    "        output['v-model'] = globals()[v_model]\n",
    "        output['violent'] = v_models\n",
    "\n",
    "        trained_datasets[model_name] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
