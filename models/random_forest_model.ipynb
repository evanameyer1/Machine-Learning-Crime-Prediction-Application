{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    def __init__(self, train_df, test_df, targets):\n",
    "        \"\"\"\n",
    "        Initialize the RandomForestModel.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The dataset for modeling.\n",
    "        - targets: The target variables to predict.\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.targets = targets\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "        self.error_metrics = None\n",
    "        self.pred = None\n",
    "        self.importances = None\n",
    "        self.trees = []\n",
    "        self.graphs = []\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Get all the instance variables in a dictionary format.\n",
    "\n",
    "        Returns:\n",
    "        - params_dict: A dictionary with instance variable names as keys and their values as values.\n",
    "        \"\"\"\n",
    "        params_dict = {\n",
    "            \"targets\": self.targets,\n",
    "            \"best_params\": self.best_params,\n",
    "            \"best_score\": self.best_score,\n",
    "            \"error_metrics\": self.error_metrics,\n",
    "            \"importances\": self.importances,\n",
    "        }\n",
    "        return params_dict\n",
    "\n",
    "    def update_targets(self, targets):\n",
    "        \"\"\"\n",
    "        Update the target variables for modeling.\n",
    "\n",
    "        Parameters:\n",
    "        - targets: The new target variables to set.\n",
    "        \"\"\"\n",
    "        self.targets = targets\n",
    "\n",
    "    def clean_data(self):\n",
    "\n",
    "        for df in [self.train_df, self.test_df]:\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_year'] = df['date'].dt.dayofyear\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def prepare_data(self, exempt=[]):\n",
    "        \"\"\"\n",
    "        Prepare the datasets for modeling.\n",
    "\n",
    "        Parameters:\n",
    "        - targets: The target variables to prepare the data for.\n",
    "\n",
    "        Returns:\n",
    "        - train_df, test_df: The prepared training and testing datasets.\n",
    "        \"\"\"\n",
    "        for target in self.targets:\n",
    "\n",
    "            for df in [self.train_df, self.test_df]:\n",
    "\n",
    "                # Shift by Date Cycles\n",
    "                df[f'{target[:2]}_next_hour'] = df[target].shift(-1)\n",
    "                df[f'{target[:2]}_next_day'] = df[target].rolling(window=24).sum()\n",
    "                df[f'{target[:2]}_next_weekday'] = df[target].rolling(window=7 * 24).sum()\n",
    "                df[f'{target[:2]}_next_month'] = df[target].rolling(window=30 * 24).sum()\n",
    "\n",
    "                # Lag by Date Cycles\n",
    "                df[f'{target[:2]}_inverse_hour'] = df[target].shift(1)\n",
    "                df[f'{target[:2]}_inverse_next_day'] = df[target].diff(24)\n",
    "                df[f'{target[:2]}_inverse_next_weekday'] = df[target].diff(7 * 24)\n",
    "                df[f'{target[:2]}_inverse_next_month'] = df[target].diff(30 * 24)\n",
    "\n",
    "                df.dropna(inplace=True)\n",
    "\n",
    "                # Rolling Mean by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_mean\"] = df[target].rolling(6).mean()\n",
    "                df[f\"{target[:2]}_12hour_mean\"] = df[target].rolling(12).mean()\n",
    "                df[f\"{target[:2]}_24hour_mean\"] = df[target].rolling(24).mean()\n",
    "                df[f\"{target[:2]}_week_mean\"] = df[target].rolling(24*7).mean()\n",
    "                df[f\"{target[:2]}_30day_mean\"] = df[target].rolling(24*30).mean()\n",
    "\n",
    "                # Rolling Min by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_min\"] = df[target].rolling(6).min()\n",
    "                df[f\"{target[:2]}_12hour_min\"] = df[target].rolling(12).min()\n",
    "                df[f\"{target[:2]}_24hour_min\"] = df[target].rolling(24).min()\n",
    "                df[f\"{target[:2]}_week_min\"] = df[target].rolling(24*7).min()\n",
    "                df[f\"{target[:2]}_30day_min\"] = df[target].rolling(24*30).min()\n",
    "\n",
    "                # Rolling Max by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_max\"] = df[target].rolling(6).max()\n",
    "                df[f\"{target[:2]}_12hour_max\"] = df[target].rolling(12).max()\n",
    "                df[f\"{target[:2]}_24hour_max\"] = df[target].rolling(24).max()\n",
    "                df[f\"{target[:2]}_week_max\"] = df[target].rolling(24*7).max()\n",
    "                df[f\"{target[:2]}_30day_max\"] = df[target].rolling(24*30).max()\n",
    "\n",
    "                # Rolling Standard Deviation by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_std\"] = df[target].rolling(6).std()\n",
    "                df[f\"{target[:2]}_12hour_std\"] = df[target].rolling(12).std()\n",
    "                df[f\"{target[:2]}_24hour_std\"] = df[target].rolling(24).std()\n",
    "                df[f\"{target[:2]}_week_std\"] = df[target].rolling(24*7).std()\n",
    "                df[f\"{target[:2]}_30day_std\"] = df[target].rolling(24*30).std()\n",
    "\n",
    "        self.features = [feature for feature in self.train_df.columns if feature not in self.targets or exempt]\n",
    "\n",
    "        return self.train_df, self.test_df, self.features\n",
    "    \n",
    "    def calculate_wae_rmse(self, reference, predictions):\n",
    "        \"\"\"\n",
    "        Calculate the Weighted Absolute Error (WAE), Root Mean Square Error (RMSE),\n",
    "        and their respective accuracies for a single target.\n",
    "\n",
    "        Parameters:\n",
    "        - reference: The reference dataset containing true values.\n",
    "        - predictions: The predicted values to compare to.\n",
    "\n",
    "        Returns:\n",
    "        - error_metrics: Dictionary containing WAE, RMSE, and their accuracies for the specified target.\n",
    "        \"\"\"\n",
    "        error_metrics = {}\n",
    "\n",
    "        # Convert DataFrame or Series to NumPy arrays\n",
    "        y_true = reference.values.astype(float)  # Convert to float if not already\n",
    "        y_pred = predictions.astype(float)\n",
    "\n",
    "        # Calculate Weighted Absolute Error (WAE)\n",
    "        wae = np.sum(np.abs(y_true - y_pred))\n",
    "        error_metrics['wae'] = wae\n",
    "\n",
    "        # Calculate Root Mean Square Error (RMSE)\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "        error_metrics['rmse'] = rmse\n",
    "\n",
    "        # Calculate the accuracy for WAE (WAE Accuracy)\n",
    "        wae_accuracy = 100 - (wae / np.sum(np.abs(y_true)) * 100)\n",
    "        error_metrics['wae_accuracy'] = wae_accuracy\n",
    "\n",
    "        # Calculate the accuracy for RMSE (RMSE Accuracy)\n",
    "        rmse_accuracy = 100 - (rmse / np.std(y_true) * 100)\n",
    "        error_metrics['rmse_accuracy'] = rmse_accuracy\n",
    "\n",
    "        self.error_metrics = error_metrics\n",
    "\n",
    "        return error_metrics\n",
    "\n",
    "    def tune_random_forest_hyperparameters(self, X, y):\n",
    "        \"\"\"\n",
    "        Tune the hyperparameters of a Random Forest model using GridSearchCV.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input features.\n",
    "        - y: Target variable.\n",
    "\n",
    "        Returns:\n",
    "        - best_params: The best hyperparameters found by GridSearchCV.\n",
    "        - best_score: The best score achieved with the best hyperparameters.\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "\n",
    "        rf = RandomForestRegressor(random_state=0, n_jobs=6)\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.best_score = grid_search.best_score_\n",
    "\n",
    "        return self.best_params, self.best_score\n",
    "\n",
    "    def fit_model(self):\n",
    "        \"\"\"\n",
    "        Fit a RandomForestRegressor model to the training dataset.\n",
    "\n",
    "        Returns:\n",
    "        - output: A dictionary containing model-related information.\n",
    "        \"\"\"\n",
    "        train_df = self.train_df.copy()\n",
    "\n",
    "        imputer = SimpleImputer()\n",
    "        self.Xtr = imputer.fit_transform(train_df[self.features])\n",
    "        self.ytr = train_df[self.targets]\n",
    "\n",
    "        best_params, best_score = self.tune_random_forest_hyperparameters(self.Xtr, self.ytr)\n",
    "\n",
    "        mdl = RandomForestRegressor(n_estimators=best_params.get('n_estimators', 100),\n",
    "                                    max_depth=best_params.get('max_depth', None),\n",
    "                                    min_samples_split=best_params.get('min_samples_split', 2),\n",
    "                                    min_samples_leaf=best_params.get('min_samples_leaf', 1),\n",
    "                                    max_features=best_params.get('max_features', 'auto'),\n",
    "                                    random_state=best_params.get('random_state', 0),\n",
    "                                    n_jobs=best_params.get('n_jobs', 6))\n",
    "\n",
    "        mdl.fit(self.Xtr, self.ytr)\n",
    "\n",
    "        self.model = mdl\n",
    "        self.params = best_params\n",
    "        self.score = best_score\n",
    "\n",
    "        return self.parameters()\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict target values for the testing dataset and compare results.\n",
    "\n",
    "        Returns:\n",
    "        - pred: Predicted values for each target column.\n",
    "        \"\"\"\n",
    "        test_df = self.test_df.copy()\n",
    "\n",
    "        # Fit the imputer on your training data first\n",
    "        imputer = SimpleImputer()\n",
    "        imputer.fit(self.train_df[self.features])\n",
    "\n",
    "        # Transform the test data using the fitted imputer\n",
    "        Xtest = imputer.transform(test_df[self.features])\n",
    "\n",
    "        # Make predictions\n",
    "        pred = self.model.predict(Xtest)\n",
    "\n",
    "        self.pred = pred\n",
    "        return self.pred\n",
    "\n",
    "    def print_performance(self, target, sig_level=0.05):\n",
    "\n",
    "        important_features = [feature for feature in self.features if self.importances.get(feature, 0) > sig_level]\n",
    "        df = self.train_df.copy().dropna(subset=self.features).fillna(0)\n",
    "        important_df = df[important_features]\n",
    "        target_df = df[target]\n",
    "\n",
    "        # Train the random forest\n",
    "        rf_most_important = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "        rf_most_important.fit(important_df, target_df)\n",
    "\n",
    "        # Prepare the test data using the same important features\n",
    "        test_important_df = df[important_features].fillna(0)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = rf_most_important.predict(test_important_df)\n",
    "\n",
    "        # Calculate WAE, RMSE, and their respective accuracies\n",
    "        error_metrics = self.calculate_wae_rmse(target_df, predictions)\n",
    "\n",
    "        # Display the performance metrics\n",
    "        print('Mean Absolute Error (WAE):', round(error_metrics['wae'], 4))\n",
    "        print('WAE Accuracy:', round(error_metrics['wae_accuracy'], 4), '%.')\n",
    "\n",
    "        print('Root Mean Square Error (RMSE):', round(error_metrics['rmse'], 4))\n",
    "        print('RMSE Accuracy:', round(error_metrics['rmse_accuracy'], 4), '%.')\n",
    "\n",
    "    def visualize_tree(self, tree_index, dot_loc, png_loc=None, new_params=None):\n",
    "        \"\"\"\n",
    "        Visualize a decision tree from the random forest model.\n",
    "\n",
    "        Args:\n",
    "            tree_index (int): Index of the tree to visualize.\n",
    "            dot_loc (str): File path to save the DOT file.\n",
    "            png_loc (str, optional): File path to save the visualization as a PNG file.\n",
    "\n",
    "        Returns:\n",
    "        - graph: A Pydot graph representing the decision tree.\n",
    "        \"\"\"\n",
    "        if new_params is not None:\n",
    "\n",
    "            tree_model = RandomForestRegressor(\n",
    "                n_estimators=new_params.get('n_estimators', self.params.get('n_estimators', 100)),\n",
    "                max_depth=new_params.get('max_depth', self.params.get('max_depth', None)),\n",
    "                min_samples_split=new_params.get('min_samples_split', self.params.get('min_samples_split', 2)),\n",
    "                min_samples_leaf=new_params.get('min_samples_leaf', self.params.get('min_samples_leaf', 1)),\n",
    "                max_features=new_params.get('max_features', self.params.get('max_features', 'auto')),\n",
    "                random_state=new_params.get('random_state', self.params.get('random_state', 0)),\n",
    "                n_jobs=new_params.get('n_jobs', self.params.get('n_jobs', 6))\n",
    "            )\n",
    "\n",
    "            tree_model.fit(self.Xtr, self.ytr)\n",
    "\n",
    "        else:\n",
    "            tree_model = self.model\n",
    "\n",
    "        if tree_index < 0 or tree_index >= len(tree_model.estimators_):\n",
    "            raise ValueError(f\"Invalid tree_index. It should be in the range [0, {len(tree_model.estimators_)-1}].\")\n",
    "        \n",
    "        tree = tree_model.estimators_[tree_index]\n",
    "\n",
    "        export_graphviz(\n",
    "            tree,\n",
    "            out_file=dot_loc,\n",
    "            feature_names=self.features,\n",
    "            rounded=True,\n",
    "            precision=1\n",
    "        )\n",
    "\n",
    "        (tree_chart, ) = pydot.graph_from_dot_file(dot_loc)\n",
    "\n",
    "        if png_loc is not None:\n",
    "            tree_chart.write_png(png_loc)\n",
    "\n",
    "        self.trees.append(tree_chart)\n",
    "\n",
    "        return tree_chart\n",
    "\n",
    "    def print_importances(self, decimal_places=4):\n",
    "\n",
    "        importances = {}\n",
    "        \n",
    "        feature_importances = [(feature, round(importance, decimal_places)) for feature, importance in zip(self.features, self.model.feature_importances_)]\n",
    "        feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for feature, importance in feature_importances:\n",
    "            importances[feature] = importance\n",
    "            print(f'Variable: {feature:20} Importance: {importance:.{decimal_places}f}')\n",
    "\n",
    "        self.importances = importances\n",
    "\n",
    "        \n",
    "    def graph_importances(self, sig_val, loc):\n",
    "        plt.style.use('ggplot')\n",
    "\n",
    "        features = [feature for feature, importance in self.importances.items() if importance > sig_val]\n",
    "        importances = [importance for feature, importance in self.importances.items() if importance > sig_val]\n",
    "\n",
    "        # List of x locations for plotting\n",
    "        x_values = list(range(len(importances)))\n",
    "\n",
    "        # Make a bar chart\n",
    "        plt.bar(x_values, importances, orientation='vertical')\n",
    "\n",
    "        # Tick labels for x axis\n",
    "        plt.xticks(x_values, features, rotation=90)\n",
    "\n",
    "        # Axis labels and title\n",
    "        plt.ylabel('Importance')\n",
    "        plt.xlabel('Variable')\n",
    "        plt.title('Variable Importances')\n",
    "\n",
    "        plt.savefig(loc)\n",
    "\n",
    "        self.graphs.append(plt)\n",
    "\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper class which implements a single tree node.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree:\n",
    "    '''\n",
    "    Class which implements a decision tree classifier algorithm.\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        '''\n",
    "        Helper function, calculates entropy from an array of integer values.\n",
    "        \n",
    "        :param s: list\n",
    "        :return: float, entropy value\n",
    "        '''\n",
    "        # Convert to integers to avoid runtime errors\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
    "\n",
    "        # Probabilities of each class label\n",
    "        percentages = counts / len(s)\n",
    "\n",
    "        # Caclulate entropy\n",
    "        entropy = 0\n",
    "        for pct in percentages:\n",
    "            if pct > 0:\n",
    "                entropy += pct * np.log2(pct)\n",
    "        return -entropy\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        '''\n",
    "        Helper function, calculates information gain from a parent and two child nodes.\n",
    "        \n",
    "        :param parent: list, the parent node\n",
    "        :param left_child: list, left child of a parent\n",
    "        :param right_child: list, right child of a parent\n",
    "        :return: float, information gain\n",
    "        '''\n",
    "        num_left = len(left_child) / len(parent)\n",
    "        num_right = len(right_child) / len(parent)\n",
    "        \n",
    "        # One-liner which implements the previously discussed formula\n",
    "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        '''\n",
    "        Helper function, calculates the best split for given features and target\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: dict\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # For every dataset feature\n",
    "        for f_idx in range(n_cols):\n",
    "\n",
    "            X_curr = X[:, f_idx]\n",
    "            # For every unique value of that feature\n",
    "            for threshold in np.unique(X_curr):\n",
    "\n",
    "                # Construct a dataset and split it to the left and right parts\n",
    "                # Left part includes records lower or equal to the threshold\n",
    "                # Right part includes records higher than the threshold\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "\n",
    "                # Do the calculation only if there's data in both subsets\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "\n",
    "                    # Obtain the value of the target variable for subsets\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "\n",
    "                    # Caclulate the information gain and save the split parameters\n",
    "                    # if the current split if better then the previous best\n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, depth=0):\n",
    "        '''\n",
    "        Helper recursive function, used to build a decision tree from the input data.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :param depth: current depth of a tree, used as a stopping criteria\n",
    "        :return: Node\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best = self._best_split(X, y)\n",
    "            # If the split isn't pure\n",
    "            if best['gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    data_left=left, \n",
    "                    data_right=right,\n",
    "                    gain=best['gain']\n",
    "                )\n",
    "            \n",
    "        # Leaf node - value is the most common target value \n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Function used to train a decision tree classifier model.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Call a recursive function to build the tree\n",
    "        self.root = self._build(X, y)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Helper recursive function, used to predict a single instance (tree traversal).\n",
    "        \n",
    "        :param x: single observation\n",
    "        :param tree: built tree\n",
    "        :return: float, predicted class\n",
    "        '''\n",
    "        # Leaf node\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # Go to the left\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "        \n",
    "        # Go to the right\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function used to classify new instances.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :return: np.array, predicted classes\n",
    "        '''\n",
    "        # Call the _predict() function for every observation\n",
    "        return [self._predict(x, self.root) for x in X]\n",
    "    \n",
    "class RandomForest:\n",
    "    '''\n",
    "    A class that implements Random Forest algorithm from scratch.\n",
    "    '''\n",
    "    def __init__(self, num_trees=25, min_samples_split=2, max_depth=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        # Will store individually trained decision trees\n",
    "        self.decision_trees = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sample(X, y):\n",
    "        '''\n",
    "        Helper function used for boostrap sampling.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array, target\n",
    "        :return: tuple (sample of features, sample of target)\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        # Sample with replacement\n",
    "        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "        return X[samples], y[samples]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains a Random Forest classifier.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Reset\n",
    "        if len(self.decision_trees) > 0:\n",
    "            self.decision_trees = []\n",
    "            \n",
    "        # Build each tree of the forest\n",
    "        num_built = 0\n",
    "        while num_built < self.num_trees:\n",
    "            try:\n",
    "                clf = DecisionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth\n",
    "                )\n",
    "                # Obtain data sample\n",
    "                _X, _y = self._sample(X, y)\n",
    "                # Train\n",
    "                clf.fit(_X, _y)\n",
    "                # Save the classifier\n",
    "                self.decision_trees.append(clf)\n",
    "                num_built += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for new data instances.\n",
    "        \n",
    "        :param X: np.array, new instances to predict\n",
    "        :return: \n",
    "        '''\n",
    "        # Make predictions with every tree in the forest\n",
    "        y = []\n",
    "        for tree in self.decision_trees:\n",
    "            y.append(tree.predict(X))\n",
    "        \n",
    "        # Reshape so we can find the most common value\n",
    "        y = np.swapaxes(a=y, axis1=0, axis2=1)\n",
    "        \n",
    "        # Use majority voting for the final prediction\n",
    "        predictions = []\n",
    "        for preds in y:\n",
    "            counter = Counter(x)\n",
    "            predictions.append(counter.most_common(1)[0][0])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = pd.read_csv('../datasets/representative_samples/test_non-violent_max.csv')\n",
    "train_sample = pd.read_csv('../datasets/representative_samples/train_non-violent_max.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestModel(train_sample, test_sample, ['non-violent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       non-violent  violent  train_rides  bike_rides  lighting  \\\n",
       " 0              0.0      1.0      39467.0        14.0      22.0   \n",
       " 1              2.0      1.0      39467.0         2.0      22.0   \n",
       " 2              0.0      2.0      39467.0         0.0      22.0   \n",
       " 3              1.0      1.0      39467.0         0.0      22.0   \n",
       " 4              1.0      0.0      39467.0         1.0      22.0   \n",
       " ...            ...      ...          ...         ...       ...   \n",
       " 25646          0.0      0.0      68996.0        12.0       0.0   \n",
       " 25647          3.0      0.0      68996.0         9.0       0.0   \n",
       " 25648          1.0      0.0      68996.0         5.0       0.0   \n",
       " 25649          0.0      0.0      68996.0         8.0       0.0   \n",
       " 25650          1.0      0.0      68996.0        17.0       0.0   \n",
       " \n",
       "        vacant_buildings  hour  day_of_week  day_of_year  month  \n",
       " 0                  31.0     1            4            1      1  \n",
       " 1                  31.0     2            4            1      1  \n",
       " 2                  31.0     3            4            1      1  \n",
       " 3                  31.0     4            4            1      1  \n",
       " 4                  31.0     5            4            1      1  \n",
       " ...                 ...   ...          ...          ...    ...  \n",
       " 25646              34.0    18            0          365     12  \n",
       " 25647              34.0    19            0          365     12  \n",
       " 25648              34.0    20            0          365     12  \n",
       " 25649              34.0    21            0          365     12  \n",
       " 25650              34.0    22            0          365     12  \n",
       " \n",
       " [25651 rows x 10 columns],\n",
       "       non-violent  violent  train_rides  bike_rides  lighting  \\\n",
       " 0             2.0      3.0      32245.0         7.0       0.0   \n",
       " 1             1.0      2.0      32245.0         6.0       0.0   \n",
       " 2             0.0      3.0      32245.0         3.0       0.0   \n",
       " 3             2.0      0.0      32245.0         0.0       0.0   \n",
       " 4             2.0      1.0      32245.0         4.0       0.0   \n",
       " ...           ...      ...          ...         ...       ...   \n",
       " 8537          0.0      0.0      98988.0        10.0       0.0   \n",
       " 8538          0.0      0.0      98988.0         7.0       0.0   \n",
       " 8539          1.0      0.0      98988.0         5.0       0.0   \n",
       " 8540          3.0      1.0      98988.0         0.0       0.0   \n",
       " 8541          1.0      1.0      79883.0         1.0       0.0   \n",
       " \n",
       "       vacant_buildings  hour  day_of_week  day_of_year  month  \n",
       " 0                 34.0     0            1            1      1  \n",
       " 1                 34.0     1            1            1      1  \n",
       " 2                 34.0     2            1            1      1  \n",
       " 3                 34.0     3            1            1      1  \n",
       " 4                 34.0     4            1            1      1  \n",
       " ...                ...   ...          ...          ...    ...  \n",
       " 8537              34.0    20            0          364     12  \n",
       " 8538              34.0    21            0          364     12  \n",
       " 8539              34.0    22            0          364     12  \n",
       " 8540              34.0    23            0          364     12  \n",
       " 8541              34.0     0            1          365     12  \n",
       " \n",
       " [8542 rows x 10 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       non-violent  violent  train_rides  bike_rides  lighting  \\\n",
       " 720            0.0      0.0      35887.0        15.0      20.0   \n",
       " 721            2.0      1.0     145929.0         1.0      20.0   \n",
       " 722            0.0      1.0     145929.0         3.0      20.0   \n",
       " 723            0.0      0.0     145929.0         2.0      20.0   \n",
       " 724            0.0      0.0     145929.0         5.0      20.0   \n",
       " ...            ...      ...          ...         ...       ...   \n",
       " 25645          2.0      0.0      68996.0        13.0       0.0   \n",
       " 25646          0.0      0.0      68996.0        12.0       0.0   \n",
       " 25647          3.0      0.0      68996.0         9.0       0.0   \n",
       " 25648          1.0      0.0      68996.0         5.0       0.0   \n",
       " 25649          0.0      0.0      68996.0         8.0       0.0   \n",
       " \n",
       "        vacant_buildings  hour  day_of_week  day_of_year  month  ...  \\\n",
       " 720                31.0    22            6           31      1  ...   \n",
       " 721                31.0     0            0           32      2  ...   \n",
       " 722                31.0     1            0           32      2  ...   \n",
       " 723                31.0     2            0           32      2  ...   \n",
       " 724                31.0     3            0           32      2  ...   \n",
       " ...                 ...   ...          ...          ...    ...  ...   \n",
       " 25645              34.0    17            0          365     12  ...   \n",
       " 25646              34.0    18            0          365     12  ...   \n",
       " 25647              34.0    19            0          365     12  ...   \n",
       " 25648              34.0    20            0          365     12  ...   \n",
       " 25649              34.0    21            0          365     12  ...   \n",
       " \n",
       "        no_6hour_max  no_12hour_max  no_24hour_max  no_week_max  no_30day_max  \\\n",
       " 720             NaN            NaN            NaN          NaN           NaN   \n",
       " 721             NaN            NaN            NaN          NaN           NaN   \n",
       " 722             NaN            NaN            NaN          NaN           NaN   \n",
       " 723             NaN            NaN            NaN          NaN           NaN   \n",
       " 724             NaN            NaN            NaN          NaN           NaN   \n",
       " ...             ...            ...            ...          ...           ...   \n",
       " 25645           5.0            5.0            5.0          8.0           8.0   \n",
       " 25646           5.0            5.0            5.0          8.0           8.0   \n",
       " 25647           5.0            5.0            5.0          8.0           8.0   \n",
       " 25648           3.0            5.0            5.0          8.0           8.0   \n",
       " 25649           3.0            5.0            5.0          8.0           8.0   \n",
       " \n",
       "        no_6hour_std  no_12hour_std  no_24hour_std  no_week_std  no_30day_std  \n",
       " 720             NaN            NaN            NaN          NaN           NaN  \n",
       " 721             NaN            NaN            NaN          NaN           NaN  \n",
       " 722             NaN            NaN            NaN          NaN           NaN  \n",
       " 723             NaN            NaN            NaN          NaN           NaN  \n",
       " 724             NaN            NaN            NaN          NaN           NaN  \n",
       " ...             ...            ...            ...          ...           ...  \n",
       " 25645      1.834848       1.537412       1.203859     1.401403      1.340326  \n",
       " 25646      1.974842       1.537412       1.209114     1.403576      1.340465  \n",
       " 25647      1.897367       1.602555       1.293798     1.403576      1.341811  \n",
       " 25648      1.211060       1.556998       1.284664     1.401453      1.341220  \n",
       " 25649      1.264911       1.602555       1.267629     1.401403      1.341811  \n",
       " \n",
       " [24930 rows x 38 columns],\n",
       "       non-violent  violent  train_rides  bike_rides  lighting  \\\n",
       " 720           0.0      0.0     134994.0         3.0       0.0   \n",
       " 721           0.0      0.0     134994.0         1.0       0.0   \n",
       " 722           0.0      0.0     134994.0         0.0       0.0   \n",
       " 723           0.0      0.0     134994.0         2.0       0.0   \n",
       " 724           0.0      0.0     134994.0        10.0       0.0   \n",
       " ...           ...      ...          ...         ...       ...   \n",
       " 8536          1.0      0.0      98988.0        18.0       0.0   \n",
       " 8537          0.0      0.0      98988.0        10.0       0.0   \n",
       " 8538          0.0      0.0      98988.0         7.0       0.0   \n",
       " 8539          1.0      0.0      98988.0         5.0       0.0   \n",
       " 8540          3.0      1.0      98988.0         0.0       0.0   \n",
       " \n",
       "       vacant_buildings  hour  day_of_week  day_of_year  month  ...  \\\n",
       " 720               34.0     0            4           32      2  ...   \n",
       " 721               34.0     1            4           32      2  ...   \n",
       " 722               34.0     2            4           32      2  ...   \n",
       " 723               34.0     3            4           32      2  ...   \n",
       " 724               34.0     4            4           32      2  ...   \n",
       " ...                ...   ...          ...          ...    ...  ...   \n",
       " 8536              34.0    19            0          364     12  ...   \n",
       " 8537              34.0    20            0          364     12  ...   \n",
       " 8538              34.0    21            0          364     12  ...   \n",
       " 8539              34.0    22            0          364     12  ...   \n",
       " 8540              34.0    23            0          364     12  ...   \n",
       " \n",
       "       no_6hour_max  no_12hour_max  no_24hour_max  no_week_max  no_30day_max  \\\n",
       " 720            NaN            NaN            NaN          NaN           NaN   \n",
       " 721            NaN            NaN            NaN          NaN           NaN   \n",
       " 722            NaN            NaN            NaN          NaN           NaN   \n",
       " 723            NaN            NaN            NaN          NaN           NaN   \n",
       " 724            NaN            NaN            NaN          NaN           NaN   \n",
       " ...            ...            ...            ...          ...           ...   \n",
       " 8536           3.0            3.0            3.0          6.0           6.0   \n",
       " 8537           3.0            3.0            3.0          6.0           6.0   \n",
       " 8538           3.0            3.0            3.0          6.0           6.0   \n",
       " 8539           2.0            3.0            3.0          6.0           6.0   \n",
       " 8540           3.0            3.0            3.0          6.0           6.0   \n",
       " \n",
       "       no_6hour_std  no_12hour_std  no_24hour_std  no_week_std  no_30day_std  \n",
       " 720            NaN            NaN            NaN          NaN           NaN  \n",
       " 721            NaN            NaN            NaN          NaN           NaN  \n",
       " 722            NaN            NaN            NaN          NaN           NaN  \n",
       " 723            NaN            NaN            NaN          NaN           NaN  \n",
       " 724            NaN            NaN            NaN          NaN           NaN  \n",
       " ...            ...            ...            ...          ...           ...  \n",
       " 8536      0.752773       0.887625       0.896854     1.071825      1.165324  \n",
       " 8537      1.048809       0.887625       0.896854     1.073038      1.165324  \n",
       " 8538      1.211060       0.887625       0.896854     1.073038      1.165324  \n",
       " 8539      0.894427       0.887625       0.883627     1.058524      1.164841  \n",
       " 8540      1.169045       0.996205       0.974308     1.068628      1.166961  \n",
       " \n",
       " [7821 rows x 38 columns],\n",
       " ['violent',\n",
       "  'train_rides',\n",
       "  'bike_rides',\n",
       "  'lighting',\n",
       "  'vacant_buildings',\n",
       "  'hour',\n",
       "  'day_of_week',\n",
       "  'day_of_year',\n",
       "  'month',\n",
       "  'no_next_hour',\n",
       "  'no_next_day',\n",
       "  'no_next_weekday',\n",
       "  'no_next_month',\n",
       "  'no_inverse_hour',\n",
       "  'no_inverse_next_day',\n",
       "  'no_inverse_next_weekday',\n",
       "  'no_inverse_next_month',\n",
       "  'no_6hour_mean',\n",
       "  'no_12hour_mean',\n",
       "  'no_24hour_mean',\n",
       "  'no_week_mean',\n",
       "  'no_30day_mean',\n",
       "  'no_6hour_min',\n",
       "  'no_12hour_min',\n",
       "  'no_24hour_min',\n",
       "  'no_week_min',\n",
       "  'no_30day_min',\n",
       "  'no_6hour_max',\n",
       "  'no_12hour_max',\n",
       "  'no_24hour_max',\n",
       "  'no_week_max',\n",
       "  'no_30day_max',\n",
       "  'no_6hour_std',\n",
       "  'no_12hour_std',\n",
       "  'no_24hour_std',\n",
       "  'no_week_std',\n",
       "  'no_30day_std'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(columns):\n",
    "  for i in columns:\n",
    "    globals()['test_' + i + '_avg'] = pd.read_csv('../datasets/representative_samples/test_' + i + '_avg.csv')\n",
    "    globals()['test_' + i + '_max'] = pd.read_csv('../datasets/representative_samples/test_' + i + '_max.csv')\n",
    "    globals()['test_' + i + '_min'] = pd.read_csv('../datasets/representative_samples/test_' + i + '_min.csv')\n",
    "    \n",
    "    globals()['train_' + i + '_avg'] = pd.read_csv('../datasets/representative_samples/train_' + i + '_avg.csv')\n",
    "    globals()['train_' + i + '_max'] = pd.read_csv('../datasets/representative_samples/train_' + i + '_max.csv')\n",
    "    globals()['train_' + i + '_min'] = pd.read_csv('../datasets/representative_samples/train_' + i + '_min.csv')\n",
    "\n",
    "    globals()[i + '_avg_model'] = RandomForestModel(globals()['train_' + i + '_avg'], globals()['test_' + i + '_avg'], [i])\n",
    "    globals()[i + '_max_model'] = RandomForestModel(globals()['train_' + i + '_max'], globals()['test_' + i + '_max'], [i])\n",
    "    globals()[i + '_min_model'] = RandomForestModel(globals()['train_' + i + '_min'], globals()['test_' + i + '_min'], [i])\n",
    "\n",
    "    globals()[i + '_avg_model'].clean_data()\n",
    "    globals()[i + '_max_model'].clean_data()\n",
    "    globals()[i + '_min_model'].clean_data()\n",
    "\n",
    "    globals()[i + '_avg_model'].prepare_data()\n",
    "    globals()[i + '_max_model'].prepare_data()\n",
    "    globals()[i + '_min_model'].prepare_data()\n",
    "\n",
    "    globals()[i + '_avg_model'].fit_model()\n",
    "    globals()[i + '_max_model'].fit_model()\n",
    "    globals()[i + '_min_model'].fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(['non-violent', 'violent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(columns):\n",
    "  for i in columns:\n",
    "    globals()[i + '_avg_model'].predict()\n",
    "    globals()[i + '_max_model'].predict()\n",
    "    globals()[i + '_min_model'].predict()\n",
    "\n",
    "    print(i + '_avg_model 0.05 accuracy')\n",
    "    globals()[i + '_avg_model'].print_performance(i)\n",
    "    print()\n",
    "    print(i + '_avg_model 0.01 accuracy')\n",
    "    globals()[i + '_avg_model'].print_performance(i, sig_level=0.01)\n",
    "    print()\n",
    "    print(i + '_avg_model all-cols accuracy')\n",
    "    globals()[i + '_avg_model'].print_performance(i, sig_level=-1)\n",
    "    print()\n",
    "    print(i + '_max_model 0.05 accuracy')\n",
    "    globals()[i + '_max_model'].print_performance(i)\n",
    "    print()\n",
    "    print(i + '_max_model 0.01 accuracy')\n",
    "    globals()[i + '_max_model'].print_performance(i, sig_level=0.01)\n",
    "    print()\n",
    "    print(i + '_max_model all-cols accuracy')\n",
    "    globals()[i + '_max_model'].print_performance(i, sig_level=-1)\n",
    "    print()\n",
    "    print(i + '_min_model 0.05 accuracy')\n",
    "    globals()[i + '_min_model'].print_performance(i)\n",
    "    print()\n",
    "    print(i + '_min_model 0.01 accuracy')\n",
    "    globals()[i + '_min_model'].print_performance(i, sig_level=0.01)\n",
    "    print()\n",
    "    print(i + '_min_model all-cols accuracy')\n",
    "    globals()[i + '_min_model'].print_performance(i, sig_level=-1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-violent_avg_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 214.1161\n",
      "WAE Accuracy: 96.9254 %.\n",
      "Root Mean Square Error (RMSE): 0.0663\n",
      "RMSE Accuracy: 88.1691 %.\n",
      "\n",
      "non-violent_avg_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 186.6052\n",
      "WAE Accuracy: 97.3204 %.\n",
      "Root Mean Square Error (RMSE): 0.0613\n",
      "RMSE Accuracy: 89.0536 %.\n",
      "\n",
      "non-violent_avg_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 92.607\n",
      "WAE Accuracy: 98.6702 %.\n",
      "Root Mean Square Error (RMSE): 0.0261\n",
      "RMSE Accuracy: 95.3372 %.\n",
      "\n",
      "non-violent_max_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 2221.5039\n",
      "WAE Accuracy: 90.5488 %.\n",
      "Root Mean Square Error (RMSE): 0.2135\n",
      "RMSE Accuracy: 83.9796 %.\n",
      "\n",
      "non-violent_max_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 1096.5036\n",
      "WAE Accuracy: 95.335 %.\n",
      "Root Mean Square Error (RMSE): 0.0986\n",
      "RMSE Accuracy: 92.6013 %.\n",
      "\n",
      "non-violent_max_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 1137.753\n",
      "WAE Accuracy: 95.1595 %.\n",
      "Root Mean Square Error (RMSE): 0.0998\n",
      "RMSE Accuracy: 92.5146 %.\n",
      "\n",
      "non-violent_min_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 214.1161\n",
      "WAE Accuracy: 96.9254 %.\n",
      "Root Mean Square Error (RMSE): 0.0663\n",
      "RMSE Accuracy: 88.1691 %.\n",
      "\n",
      "non-violent_min_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 186.6052\n",
      "WAE Accuracy: 97.3204 %.\n",
      "Root Mean Square Error (RMSE): 0.0613\n",
      "RMSE Accuracy: 89.0536 %.\n",
      "\n",
      "non-violent_min_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 92.607\n",
      "WAE Accuracy: 98.6702 %.\n",
      "Root Mean Square Error (RMSE): 0.0261\n",
      "RMSE Accuracy: 95.3372 %.\n",
      "\n",
      "violent_avg_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 13.7883\n",
      "WAE Accuracy: 99.6041 %.\n",
      "Root Mean Square Error (RMSE): 0.0161\n",
      "RMSE Accuracy: 95.8508 %.\n",
      "\n",
      "violent_avg_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 9.1603\n",
      "WAE Accuracy: 99.737 %.\n",
      "Root Mean Square Error (RMSE): 0.0126\n",
      "RMSE Accuracy: 96.7548 %.\n",
      "\n",
      "violent_avg_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 6.217\n",
      "WAE Accuracy: 99.8215 %.\n",
      "Root Mean Square Error (RMSE): 0.0068\n",
      "RMSE Accuracy: 98.2405 %.\n",
      "\n",
      "violent_max_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 3296.732\n",
      "WAE Accuracy: 81.0913 %.\n",
      "Root Mean Square Error (RMSE): 0.264\n",
      "RMSE Accuracy: 69.7077 %.\n",
      "\n",
      "violent_max_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 1145.8167\n",
      "WAE Accuracy: 93.4281 %.\n",
      "Root Mean Square Error (RMSE): 0.1055\n",
      "RMSE Accuracy: 87.8967 %.\n",
      "\n",
      "violent_max_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 1094.99\n",
      "WAE Accuracy: 93.7196 %.\n",
      "Root Mean Square Error (RMSE): 0.0915\n",
      "RMSE Accuracy: 89.5076 %.\n",
      "\n",
      "violent_min_model 0.05 accuracy\n",
      "Mean Absolute Error (WAE): 13.7883\n",
      "WAE Accuracy: 99.6041 %.\n",
      "Root Mean Square Error (RMSE): 0.0161\n",
      "RMSE Accuracy: 95.8508 %.\n",
      "\n",
      "violent_min_model 0.01 accuracy\n",
      "Mean Absolute Error (WAE): 9.1603\n",
      "WAE Accuracy: 99.737 %.\n",
      "Root Mean Square Error (RMSE): 0.0126\n",
      "RMSE Accuracy: 96.7548 %.\n",
      "\n",
      "violent_min_model all-cols accuracy\n",
      "Mean Absolute Error (WAE): 6.217\n",
      "WAE Accuracy: 99.8215 %.\n",
      "Root Mean Square Error (RMSE): 0.0068\n",
      "RMSE Accuracy: 98.2405 %.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_predictions(['non-violent', 'violent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAE = 97.65%\n",
    "# RMSE = 94.86%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
