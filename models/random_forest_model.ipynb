{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Initialize the RandomForestModel.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The dataset for modeling.\n",
    "        - targets: The target variables to predict.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "        self.error_metrics = None\n",
    "        self.pred = None\n",
    "        self.importances = None\n",
    "        self.trees = []\n",
    "        self.graphs = []\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Get all the instance variables in a dictionary format.\n",
    "\n",
    "        Returns:\n",
    "        - params_dict: A dictionary with instance variable names as keys and their values as values.\n",
    "        \"\"\"\n",
    "        params_dict = {\n",
    "            \"targets\": self.targets,\n",
    "            \"best_params\": self.best_params,\n",
    "            \"best_score\": self.best_score,\n",
    "            \"error_metrics\": self.error_metrics,\n",
    "            \"importances\": self.importances,\n",
    "        }\n",
    "        return params_dict\n",
    "\n",
    "    def update_targets(self, targets):\n",
    "        \"\"\"\n",
    "        Update the target variables for modeling.\n",
    "\n",
    "        Parameters:\n",
    "        - targets: The new target variables to set.\n",
    "        \"\"\"\n",
    "        self.targets = targets\n",
    "\n",
    "    def update_data(self, data):\n",
    "        \"\"\"\n",
    "        Update the dataset for modeling.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The new dataset to set.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def clean_data(self):\n",
    "\n",
    "        for df in [self.train_df, self.test_df]:\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_year'] = df['date'].dt.dayofyear\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def prepare_data(self, exempt=[]):\n",
    "        \"\"\"\n",
    "        Prepare the datasets for modeling.\n",
    "\n",
    "        Parameters:\n",
    "        - targets: The target variables to prepare the data for.\n",
    "\n",
    "        Returns:\n",
    "        - train_df, test_df: The prepared training and testing datasets.\n",
    "        \"\"\"\n",
    "        for target in self.targets:\n",
    "\n",
    "            for df in [self.train_df, self.test_df]:\n",
    "\n",
    "                # Shift by Date Cycles\n",
    "                df[f'{target[:2]}_next_hour'] = df[target].shift(-1)\n",
    "                df[f'{target[:2]}_next_day'] = df[target].rolling(window=24).sum()\n",
    "                df[f'{target[:2]}_next_weekday'] = df[target].rolling(window=7 * 24).sum()\n",
    "                df[f'{target[:2]}_next_month'] = df[target].rolling(window=30 * 24).sum()\n",
    "\n",
    "                # Lag by Date Cycles\n",
    "                df[f'{target[:2]}_inverse_hour'] = df[target].shift(1)\n",
    "                df[f'{target[:2]}_inverse_next_day'] = df[target].diff(24)\n",
    "                df[f'{target[:2]}_inverse_next_weekday'] = df[target].diff(7 * 24)\n",
    "                df[f'{target[:2]}_inverse_next_month'] = df[target].diff(30 * 24)\n",
    "\n",
    "                df.dropna(inplace=True)\n",
    "\n",
    "                # Rolling Mean by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_mean\"] = df[target].rolling(6).mean()\n",
    "                df[f\"{target[:2]}_12hour_mean\"] = df[target].rolling(12).mean()\n",
    "                df[f\"{target[:2]}_24hour_mean\"] = df[target].rolling(24).mean()\n",
    "                df[f\"{target[:2]}_week_mean\"] = df[target].rolling(24*7).mean()\n",
    "                df[f\"{target[:2]}_30day_mean\"] = df[target].rolling(24*30).mean()\n",
    "\n",
    "                # Rolling Min by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_min\"] = df[target].rolling(6).min()\n",
    "                df[f\"{target[:2]}_12hour_min\"] = df[target].rolling(12).min()\n",
    "                df[f\"{target[:2]}_24hour_min\"] = df[target].rolling(24).min()\n",
    "                df[f\"{target[:2]}_week_min\"] = df[target].rolling(24*7).min()\n",
    "                df[f\"{target[:2]}_30day_min\"] = df[target].rolling(24*30).min()\n",
    "\n",
    "                # Rolling Max by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_max\"] = df[target].rolling(6).max()\n",
    "                df[f\"{target[:2]}_12hour_max\"] = df[target].rolling(12).max()\n",
    "                df[f\"{target[:2]}_24hour_max\"] = df[target].rolling(24).max()\n",
    "                df[f\"{target[:2]}_week_max\"] = df[target].rolling(24*7).max()\n",
    "                df[f\"{target[:2]}_30day_max\"] = df[target].rolling(24*30).max()\n",
    "\n",
    "                # Rolling Standard Deviation by Date Cycles\n",
    "                df[f\"{target[:2]}_6hour_std\"] = df[target].rolling(6).std()\n",
    "                df[f\"{target[:2]}_12hour_std\"] = df[target].rolling(12).std()\n",
    "                df[f\"{target[:2]}_24hour_std\"] = df[target].rolling(24).std()\n",
    "                df[f\"{target[:2]}_week_std\"] = df[target].rolling(24*7).std()\n",
    "                df[f\"{target[:2]}_30day_std\"] = df[target].rolling(24*30).std()\n",
    "\n",
    "        self.features = [feature for feature in self.train_df.columns if feature not in self.targets or exempt]\n",
    "\n",
    "        return self.train_df, self.test_df, self.features\n",
    "    \n",
    "    def calculate_wae_rmse(self, reference, predictions):\n",
    "        \"\"\"\n",
    "        Calculate the Weighted Absolute Error (WAE), Root Mean Square Error (RMSE),\n",
    "        and their respective accuracies for a single target.\n",
    "\n",
    "        Parameters:\n",
    "        - reference: The reference dataset containing true values.\n",
    "        - predictions: The predicted values to compare to.\n",
    "\n",
    "        Returns:\n",
    "        - error_metrics: Dictionary containing WAE, RMSE, and their accuracies for the specified target.\n",
    "        \"\"\"\n",
    "        error_metrics = {}\n",
    "    \n",
    "        # Convert DataFrame or Series to NumPy arrays\n",
    "        y_true = reference.values\n",
    "        y_pred = predictions.values if isinstance(predictions, pd.Series) else predictions\n",
    "\n",
    "        # Calculate Weighted Absolute Error (WAE)\n",
    "        wae = np.sum(np.abs(y_true - y_pred))\n",
    "        error_metrics['wae'] = wae\n",
    "\n",
    "        # Calculate Root Mean Square Error (RMSE)\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "        error_metrics['rmse'] = rmse\n",
    "\n",
    "        # Calculate the accuracy for WAE (WAE Accuracy)\n",
    "        wae_accuracy = 100 - (wae / np.sum(np.abs(y_true)) * 100)\n",
    "        error_metrics['wae_accuracy'] = wae_accuracy\n",
    "\n",
    "        # Calculate the accuracy for RMSE (RMSE Accuracy)\n",
    "        rmse_accuracy = 100 - (rmse / np.std(y_true) * 100)\n",
    "        error_metrics['rmse_accuracy'] = rmse_accuracy\n",
    "\n",
    "        self.error_metrics = error_metrics\n",
    "\n",
    "        return error_metrics\n",
    "\n",
    "    def tune_random_forest_hyperparameters(self, X, y):\n",
    "        \"\"\"\n",
    "        Tune the hyperparameters of a Random Forest model using GridSearchCV.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input features.\n",
    "        - y: Target variable.\n",
    "\n",
    "        Returns:\n",
    "        - best_params: The best hyperparameters found by GridSearchCV.\n",
    "        - best_score: The best score achieved with the best hyperparameters.\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "\n",
    "        rf = RandomForestRegressor(random_state=0, n_jobs=6)\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.best_score = grid_search.best_score_\n",
    "\n",
    "        return self.best_params, self.best_score\n",
    "\n",
    "    def fit_model(self):\n",
    "        \"\"\"\n",
    "        Fit a RandomForestRegressor model to the training dataset.\n",
    "\n",
    "        Returns:\n",
    "        - output: A dictionary containing model-related information.\n",
    "        \"\"\"\n",
    "        train_df = self.train_df.copy()\n",
    "\n",
    "        imputer = SimpleImputer()\n",
    "        self.Xtr = imputer.fit_transform(train_df[self.features])\n",
    "        self.ytr = train_df[self.targets]\n",
    "\n",
    "        best_params, best_score = self.tune_random_forest_hyperparameters(self.Xtr, self.ytr)\n",
    "\n",
    "        mdl = RandomForestRegressor(n_estimators=best_params.get('n_estimators', 100),\n",
    "                                    max_depth=best_params.get('max_depth', None),\n",
    "                                    min_samples_split=best_params.get('min_samples_split', 2),\n",
    "                                    min_samples_leaf=best_params.get('min_samples_leaf', 1),\n",
    "                                    max_features=best_params.get('max_features', 'auto'),\n",
    "                                    random_state=best_params.get('random_state', 0),\n",
    "                                    n_jobs=best_params.get('n_jobs', 6))\n",
    "\n",
    "        mdl.fit(self.Xtr, self.ytr)\n",
    "\n",
    "        self.model = mdl\n",
    "        self.params = best_params\n",
    "        self.score = best_score\n",
    "\n",
    "        return self.parameters()\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict target values for the testing dataset and compare results.\n",
    "\n",
    "        Returns:\n",
    "        - pred: Predicted values for each target column.\n",
    "        \"\"\"\n",
    "        test_df = self.test_df.copy()\n",
    "\n",
    "        # Fit the imputer on your training data first\n",
    "        imputer = SimpleImputer()\n",
    "        imputer.fit(self.train_df[self.features])\n",
    "\n",
    "        # Transform the test data using the fitted imputer\n",
    "        Xtest = imputer.transform(test_df[self.features])\n",
    "\n",
    "        # Make predictions\n",
    "        pred = self.model.predict(Xtest)\n",
    "\n",
    "        self.pred = pred\n",
    "        return self.pred\n",
    "\n",
    "    def print_performance(self, target, sig_level=0.05):\n",
    "\n",
    "        important_features = [feature for feature in self.features if self.importances.get(feature, 0) > sig_level]\n",
    "        df = self.train_df.copy().dropna(subset=self.features).fillna(0)\n",
    "        important_df = df[important_features]\n",
    "        target_df = df[target]\n",
    "\n",
    "        # Train the random forest\n",
    "        rf_most_important = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "        rf_most_important.fit(important_df, target_df)\n",
    "\n",
    "        # Prepare the test data using the same important features\n",
    "        test_important_df = df[important_features].fillna(0)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = rf_most_important.predict(test_important_df)\n",
    "\n",
    "        print(predictions)\n",
    "\n",
    "        # Calculate WAE, RMSE, and their respective accuracies\n",
    "        error_metrics = self.calculate_wae_rmse(target_df, target)\n",
    "\n",
    "        # Display the performance metrics\n",
    "        print('Mean Absolute Error (WAE):', round(error_metrics['wae'], 4))\n",
    "        print('WAE Accuracy:', round(error_metrics['wae_accuracy'], 4), '%.')\n",
    "\n",
    "        print('Root Mean Square Error (RMSE):', round(error_metrics['rmse'], 4))\n",
    "        print('RMSE Accuracy:', round(error_metrics['rmse_accuracy'], 4), '%.')\n",
    "\n",
    "    def visualize_tree(self, tree_index, dot_loc, png_loc=None, new_params=None):\n",
    "        \"\"\"\n",
    "        Visualize a decision tree from the random forest model.\n",
    "\n",
    "        Args:\n",
    "            tree_index (int): Index of the tree to visualize.\n",
    "            dot_loc (str): File path to save the DOT file.\n",
    "            png_loc (str, optional): File path to save the visualization as a PNG file.\n",
    "\n",
    "        Returns:\n",
    "        - graph: A Pydot graph representing the decision tree.\n",
    "        \"\"\"\n",
    "        if new_params is not None:\n",
    "\n",
    "            tree_model = RandomForestRegressor(\n",
    "                n_estimators=new_params.get('n_estimators', self.params.get('n_estimators', 100)),\n",
    "                max_depth=new_params.get('max_depth', self.params.get('max_depth', None)),\n",
    "                min_samples_split=new_params.get('min_samples_split', self.params.get('min_samples_split', 2)),\n",
    "                min_samples_leaf=new_params.get('min_samples_leaf', self.params.get('min_samples_leaf', 1)),\n",
    "                max_features=new_params.get('max_features', self.params.get('max_features', 'auto')),\n",
    "                random_state=new_params.get('random_state', self.params.get('random_state', 0)),\n",
    "                n_jobs=new_params.get('n_jobs', self.params.get('n_jobs', 6))\n",
    "            )\n",
    "\n",
    "            tree_model.fit(self.Xtr, self.ytr)\n",
    "\n",
    "        else:\n",
    "            tree_model = self.model\n",
    "\n",
    "        if tree_index < 0 or tree_index >= len(tree_model.estimators_):\n",
    "            raise ValueError(f\"Invalid tree_index. It should be in the range [0, {len(tree_model.estimators_)-1}].\")\n",
    "        \n",
    "        tree = tree_model.estimators_[tree_index]\n",
    "\n",
    "        export_graphviz(\n",
    "            tree,\n",
    "            out_file=dot_loc,\n",
    "            feature_names=self.features,\n",
    "            rounded=True,\n",
    "            precision=1\n",
    "        )\n",
    "\n",
    "        (tree_chart, ) = pydot.graph_from_dot_file(dot_loc)\n",
    "\n",
    "        if png_loc is not None:\n",
    "            tree_chart.write_png(png_loc)\n",
    "\n",
    "        self.trees.append(tree_chart)\n",
    "\n",
    "        return tree_chart\n",
    "\n",
    "    def print_importances(self, decimal_places=4):\n",
    "\n",
    "        importances = {}\n",
    "        \n",
    "        feature_importances = [(feature, round(importance, decimal_places)) for feature, importance in zip(self.features, self.model.feature_importances_)]\n",
    "        feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for feature, importance in feature_importances:\n",
    "            importances[feature] = importance\n",
    "            print(f'Variable: {feature:20} Importance: {importance:.{decimal_places}f}')\n",
    "\n",
    "        self.importances = importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_importances(self, sig_val, loc):\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    features = [feature for feature, importance in self.importances.items() if importance > sig_val]\n",
    "    importances = [importance for feature, importance in self.importances.items() if importance > sig_val]\n",
    "\n",
    "    # List of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "\n",
    "    # Make a bar chart\n",
    "    plt.bar(x_values, importances, orientation='vertical')\n",
    "\n",
    "    # Tick labels for x axis\n",
    "    plt.xticks(x_values, features, rotation=90)\n",
    "\n",
    "    # Axis labels and title\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Variable')\n",
    "    plt.title('Variable Importances')\n",
    "\n",
    "    plt.savefig(loc)\n",
    "\n",
    "    self.graphs.append(plt)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper class which implements a single tree node.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree:\n",
    "    '''\n",
    "    Class which implements a decision tree classifier algorithm.\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        '''\n",
    "        Helper function, calculates entropy from an array of integer values.\n",
    "        \n",
    "        :param s: list\n",
    "        :return: float, entropy value\n",
    "        '''\n",
    "        # Convert to integers to avoid runtime errors\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
    "\n",
    "        # Probabilities of each class label\n",
    "        percentages = counts / len(s)\n",
    "\n",
    "        # Caclulate entropy\n",
    "        entropy = 0\n",
    "        for pct in percentages:\n",
    "            if pct > 0:\n",
    "                entropy += pct * np.log2(pct)\n",
    "        return -entropy\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        '''\n",
    "        Helper function, calculates information gain from a parent and two child nodes.\n",
    "        \n",
    "        :param parent: list, the parent node\n",
    "        :param left_child: list, left child of a parent\n",
    "        :param right_child: list, right child of a parent\n",
    "        :return: float, information gain\n",
    "        '''\n",
    "        num_left = len(left_child) / len(parent)\n",
    "        num_right = len(right_child) / len(parent)\n",
    "        \n",
    "        # One-liner which implements the previously discussed formula\n",
    "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        '''\n",
    "        Helper function, calculates the best split for given features and target\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: dict\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # For every dataset feature\n",
    "        for f_idx in range(n_cols):\n",
    "\n",
    "            X_curr = X[:, f_idx]\n",
    "            # For every unique value of that feature\n",
    "            for threshold in np.unique(X_curr):\n",
    "\n",
    "                # Construct a dataset and split it to the left and right parts\n",
    "                # Left part includes records lower or equal to the threshold\n",
    "                # Right part includes records higher than the threshold\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "\n",
    "                # Do the calculation only if there's data in both subsets\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "\n",
    "                    # Obtain the value of the target variable for subsets\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "\n",
    "                    # Caclulate the information gain and save the split parameters\n",
    "                    # if the current split if better then the previous best\n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, depth=0):\n",
    "        '''\n",
    "        Helper recursive function, used to build a decision tree from the input data.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :param depth: current depth of a tree, used as a stopping criteria\n",
    "        :return: Node\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best = self._best_split(X, y)\n",
    "            # If the split isn't pure\n",
    "            if best['gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    data_left=left, \n",
    "                    data_right=right, \n",
    "                    gain=best['gain']\n",
    "                )\n",
    "            \n",
    "        # Leaf node - value is the most common target value \n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Function used to train a decision tree classifier model.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array or list, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Call a recursive function to build the tree\n",
    "        self.root = self._build(X, y)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Helper recursive function, used to predict a single instance (tree traversal).\n",
    "        \n",
    "        :param x: single observation\n",
    "        :param tree: built tree\n",
    "        :return: float, predicted class\n",
    "        '''\n",
    "        # Leaf node\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # Go to the left\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "        \n",
    "        # Go to the right\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function used to classify new instances.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :return: np.array, predicted classes\n",
    "        '''\n",
    "        # Call the _predict() function for every observation\n",
    "        return [self._predict(x, self.root) for x in X]\n",
    "    \n",
    "class RandomForest:\n",
    "    '''\n",
    "    A class that implements Random Forest algorithm from scratch.\n",
    "    '''\n",
    "    def __init__(self, num_trees=25, min_samples_split=2, max_depth=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        # Will store individually trained decision trees\n",
    "        self.decision_trees = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sample(X, y):\n",
    "        '''\n",
    "        Helper function used for boostrap sampling.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array, target\n",
    "        :return: tuple (sample of features, sample of target)\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        # Sample with replacement\n",
    "        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "        return X[samples], y[samples]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains a Random Forest classifier.\n",
    "        \n",
    "        :param X: np.array, features\n",
    "        :param y: np.array, target\n",
    "        :return: None\n",
    "        '''\n",
    "        # Reset\n",
    "        if len(self.decision_trees) > 0:\n",
    "            self.decision_trees = []\n",
    "            \n",
    "        # Build each tree of the forest\n",
    "        num_built = 0\n",
    "        while num_built < self.num_trees:\n",
    "            try:\n",
    "                clf = DecisionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth\n",
    "                )\n",
    "                # Obtain data sample\n",
    "                _X, _y = self._sample(X, y)\n",
    "                # Train\n",
    "                clf.fit(_X, _y)\n",
    "                # Save the classifier\n",
    "                self.decision_trees.append(clf)\n",
    "                num_built += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for new data instances.\n",
    "        \n",
    "        :param X: np.array, new instances to predict\n",
    "        :return: \n",
    "        '''\n",
    "        # Make predictions with every tree in the forest\n",
    "        y = []\n",
    "        for tree in self.decision_trees:\n",
    "            y.append(tree.predict(X))\n",
    "        \n",
    "        # Reshape so we can find the most common value\n",
    "        y = np.swapaxes(a=y, axis1=0, axis2=1)\n",
    "        \n",
    "        # Use majority voting for the final prediction\n",
    "        predictions = []\n",
    "        for preds in y:\n",
    "            counter = Counter(x)\n",
    "            predictions.append(counter.most_common(1)[0][0])\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
